Check the preprocessed data
===========================

After the script generated by uber_subject.py has completed, navigate to the directory containing the preprocessed data. By default, AFNI will create a new directory tree in the following format:

cd sub-02/subject_results/group.BART/subj.sub_02/sub_02.results

In which subjects name and group name are specified in the subject ID and group ID tab from the uber_subject.py GUI we did. 

Look at the Preprocessed files
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

You can see all the different images and files from each step of preprocessing. Especially, these files starting from "pb" (Processing Block) are preprocessed functional images and the files with "T1w" 
are the preprocessed anatomical images. 

.. figure:: AFNI_subje_resut.png

  The files containing the “pb” are the preprocessed functional images, the files with the “T1w” string are the preprocessed anatomical images. the name ""3dTshift"" means that these images have been 
  slice-time corrected by the "3dTshift" command.

Processed Functional Image
^^^^^^^^^^^^^^^^^^^^^^^^^^

After check with different files in the preprocessed data directory, let's tale a close look, type ``afni`` to open the AFNI GUI. Click the "Underlay", and choose "pb00.sub_02.r01.tcat", click on the 
""Graph"" next to any of the Axial, Sagittal, or Coronal views to view the time-series. You also can see the same image when you open "pb00.sub_02.r02.tcat" and "pb00.sub_02.r03.tcat"; it is because the 
initial volumes of dataset had had been in OpenNeuro. 

The **Underlay** menu has two columns: The left column is the file name, and the right contains header information about the file. “epan” indicates that it is an echo-planar image (functional image as we 
introduced from last chapter), whereas “abuc” stands for a anatomical image. “3D+t:300” indicates that it is a 3-dimensional with 300 volumes (time points) image


.. image:: AFNI_underlay_view.png

.. image:: AFNI_timeseries.png

Aligned and Co-Registered image
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The next file to look at is the "pb02.sub_02.r01.volreg+tlrc.BRIK", which has 3 meanings:

1 Motion-corrected, each volume in the time-series for this run has been aligned to a reference volume. 

2 Co-registered to the anatomical image, the functional image has registered into anatomical image.

3 the images also have been normalized to a standardized space, which is MNI152 template.

If you click on the pb02... images, you will notice that there is a section of the AFNI GUI that has “Original View”, “AC-PC Aligned”, and “Talairach View”. In this images, the “Talairach View” is 
highlighted, which indicated that these images have been normalized. you can click the different locations of the images to see the differences.

.. image:: AFNI_coregister.png

Smoothed image
^^^^^^^^^^^^^^

The following preprocessing step is smoothing, which averages the signal of nearby voxels together in order to boost any signal that is there, and to cancel out noise. These images will look more blurry 
as a function of the size of the smoothing kernel that you apply to the data; in this case, a smoothing kernel of 4mm will blur the data slightly, but not by much. Look at the images to make sure that 
the blurring looks reasonable, as in the figure below.

.. image:: AFNI_smooth.png

Scaled image
^^^^^^^^^^^^

The last preprocessing step generates scaled images, in which each voxel has a mean signal intensity of 100. This allows us to specify any changes relative to the mean as percent signal change; i.e., a 
value of 101 could be interpreted as a signal change of 1%.

Due to the greyscale of the images being more uniform in the brain voxels as compared to greater variability in the signal outside of the brain, these images will have less anatomical definition than the 
previous images. Nevertheless, you should still be able to see the outline of the brain, and the time-series values of the brain voxels should all be close to 100:

the Masks
^^^^^^^^^

Because we are interested only in the voxels covering the brain, we created a mask that we can use to exclude any non-brain voxels. The mask will be binary: 1’s in the voxels that are determined to be 
within the skull, and 0’s outside of the skull. (More rigorous masks can be created which will also exclude cerebrospinal fluid and even white matter, but we are not considering those here.)

There are two masks that you can choose between: full_mask and mask_group. The full_mask image is a union of all of the individual functional image masks, which have been determined to belong to the 
brain based on their signal intensity. Voxels with very low signal intensity are not considered brain voxels. As you can see with the full_mask image, this also excludes voxels in the orbitofrontal area, 
which is notorious for being susceptible to signal dropout: 

The other mask, mask_group, is a more liberal mask that has been dilated to more closely match the template that you have warped to - in this case, the MNI152 brain:


Anatomical Images
^^^^^^^^^^^^^^^^^

When viewing the results of the anatomical preprocessing, we will want to make sure that both the skull-stripping looks reasonable and that the images were normalized properly.

First, open the image anat_w_skull_warped. If you have copied the MNI152 image into the aglobal directory, load it as an overlay image. (You can also copy it into the current directory by typing from the 
Terminal: cp ~/abin/MNI_avg152T1+tlrc* ..) You may notice that while the sagittal view looks fine, the axial and coronal views look worse. In particular, it looks as though the image is slightly shifted 
to the right. Although it is common to have some variability in normalization, and that the anatomical and the template will never match perfectly, this is beyond the margin of error we are willing to 
extend to normalization.

The anat_w_skull_warped image, it should be noted, is the result of a warp being applied to the raw anatomical image. The warp itself was computed by normalizing the skull-stripped anatomical to a 
template. If that normalization was off somehow, it would have propagated to the other images. To check this, load as an underlay the image anat_final:

We have found the source of the error: Part of the brain on the left has been removed during normalization. But how do we fix this?

When you detect an error in the preprocessed images, you should examine the output of your preprocessing script. If you started the script from the uber_subject.py GUI, the output will be printed to the 
“Processing Command” window; a copy of the text will also be stored in a file called output.proc.<subjID, which is located one directory above the preprocessed data.

This text will contain both Warnings and Errors. Errors indicate that either a file is missing, or a command was not able to run successfully. Usually the script will exit after an error is encountered. 
Warnings, on the other hand, point out something that may be a problem. An example of a warning is the “dataset already aligned in time” notification that we received during slice-timing correction.

Another Warning, related to our current problem, occurred during the normalization step. This can be found slightly after halfway down the output, after the command @auto_tlrc:

Apparently the centers of the anatomical and template images are very far apart. The output says that “if parts of the orignal anatomy gets cropped [sic]” (which is our current problem), “try adding 
option -init_xform AUTO_CENTER to your @auto_tlrc command.” We can do so by navigating to one directory above the preprocessing directory (cd ..), removing the preprocessing directory (rm -r 
sub_08.results), and editing the file proc.sub_08 to include the string -init_xform AUTO_CENTER after the @auto_tlrc command, which should be line 119 in your proc file:

Save the file, and rerun it by typing tcsh proc.sub_08. Wait a few minutes for it to finish, and then navigate into the preprocessing directory and load the same set of images as before. You should now 
see that the problem is fixed:
